{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 1 - Search.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "65F-jXa1BjUx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KAmbuske02/cis490/blob/main/Project_1_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1 - __*SEARCH*__ (150 pts total)\n",
        "\n",
        "<font color=\"red\">__*PLEASE:* YOU MUST WRITE YOUR OWN CODE, YOU MAY NOT SHARE CODE, YOU CAN DISCUSS WITH EACH OTHER, YOU CAN USE ANY RESOURCE ON THE INTERNET__</font>‚ùó\n",
        "\n",
        "<img src=\"https://img.freepik.com/free-vector/black-robot-holding-magnifying-glass-data-search-artificial-intelligence_48369-37442.jpg?size=338&ext=jpg\" width=200>\n",
        "\n",
        "Trying to find an optimal strategy for an agent can be viewed as a search through a [**space**](https://www.youtube.com/watch?v=BVn1oQL9sWg&t=44s) of possible solutions. For example, one type of open loop solution in MountainCar\n",
        "that we discussed, involved:\n",
        "\n",
        "  * First going left (i.e. reverse or action = 0) for a fixed number of steps (*num_left*).\n",
        "  * Then, going right (action = 2) until done\n",
        "\n",
        "The search space in this case becomes a one dimensional search for the ideal number of left steps to take (**num_left**) before going right. In other words, the problem is: find the *optimal* **num_left**, where optimal means it gets the car to the flag in the lowest number of steps on average.  \n",
        "\n",
        "Because the MountainCar environment has a random element (i.e. the car starts in a slightly random position near the bottom of the hill), we would need to test each option of **num_left** several times in order to calculate an average. In problems where a large number of steps can be easily done, we can simply use a **brute force** - style of search, i.e. try every possible solution to find the best.\n",
        "\n",
        "When the problem is such that it is too computationally intensive to perform a brute force search in a reasonable amount of time, we can use a variety of **local search** algorithms. **local search** refers generally to the concept that you evaluate a single point in the solution space, (the first point is usually random, lets call it Solution zero, or $S_0$), then take a next step to a new spot in the solution space (lets call it $S_1$) that is *fairly* close (i.e. local) to the spot you were just at. By *evaluate* we mean calculate or estimate some cost or benefit. For MountainCar, the cost is the number of steps (i.e. delay) that it takes for the car to reach the flag. For some problems, there is a direct formula for evaluating the cost as well as the slope of the cost with respect to one of the variables. There is no such formula for MountainCar, we are at the mercy of running the simulation environment serveral times to evaluate the average delay for a givein solution, and have no direct way to calcuate how muuch the delay will go down or up if we increase or decreate the number of left steps from a solution we just evaluated. In these types of problems, we typically use **randomized local search**, that is, sthe next step we take in searching the solution space ($S_{t+1}$, where t is time, t+1 is the next step) is to take a random step in a random direction. If the new solution space spot $S+{t+1}$ is better than the last $S_t$, we continue along this \"path\", and take the next next random step $S_{t+2}$ a random step away from $S_{t+1}$. If the new spot $S_{t+1}$ is worse than $S_t$, we \"go back\" to $S_t$ and try a new random step $S_{t+1'}$ (note the prime)in the solution space. **hill climbing** is the general term for accepting a new step if it is better than the last. \n",
        "\n",
        "One particularly intersting randomized local search technique is analogized with the process of *annealing metal*, i.e heating up metal and slowly cooling it down to encourage a *lower-energy, more-stable state* in which crystallization with a low number of deformtities is encouraged. In **simulated annealing**, we modify hill climbing such that, if the next solution $S_{t+1}$ evaluated is worse than the last $S_{t}$, we flip a loaded coin to decide if we accept the worse solution $S_{t+1}$ and continue the search from there, or go back to the previous better solution $S_{t}$.\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcS1L50SeRkXL_rL21NWTQ_yTP1reDLi5okSnkGag_aO4umzpMfdck9mnpGNKngswXz-udc&usqp=CAU\">"
      ],
      "metadata": {
        "id": "dNSc4S5MZp1G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part I QUESTIONS (50 pts)\n",
        "\n",
        "For each, please give a one sentence answer.\n",
        "\n",
        "**What are the differences from breadth-first search and depth-first search?** Breadth-first search starts from the top node and works its way down, whereas Depth-first search searches as far down a node line as it can before backtracking.\n",
        "\n",
        "**What is brute force search?** Brute force search is trying every possible candidate for a solution until it finds the best/right one.\n",
        "\n",
        "**What is hill climbing?** Hill climbing is a search algorithm that continuously moves \"upward\" until it finds the \"peak\"\n",
        "\n",
        "**What is beam search?** Beam search searches for the best candidate nodes, looks into those nodes, picks the best candidates from those nodes and so on.\n"
      ],
      "metadata": {
        "id": "65F-jXa1BjUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part II - Brute force search of 1d in Mountain Car (25pts)::\n",
        "\n"
      ],
      "metadata": {
        "id": "sGUvilz2WUW7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "yXv6kHBxWPRF",
        "outputId": "838728da-e997-46f9-c40a-c6d49a492143"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# brute force search of one type of MountainCar agent: an\n",
        "# agent that goes a number of lefts first, then right until done\n",
        "import gym\n",
        "import matplotlib.pyplot as plt  # This import is for plotting (2d graph)\n",
        "\n",
        "num_repeats = 1000  # The number of times gym env is run for each search point\n",
        "search_space = list(range(25,50))  # All potential #lefts before going right\n",
        "avg_score_list = [] \n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "for num_lefts in search_space:\n",
        "  #print('#lefts = ', num_lefts, end='')\n",
        "  result_list = []\n",
        "  for iteration in range(num_repeats):\n",
        "    observation = env.reset()\n",
        "    step_i = 0  # use this variable as a counter for each step taken\n",
        "    while True:    \n",
        "      if step_i < num_lefts:\n",
        "        action = 0\n",
        "      else:\n",
        "        action = 2\n",
        "      observation, reward, done, info = env.step(action) \n",
        "      step_i = step_i + 1\n",
        "\n",
        "      # if car reaches the flag, exit the loop\n",
        "      if done: \n",
        "        break;\n",
        "    result_list.append(step_i)\n",
        "  score = sum(result_list)/num_repeats\n",
        "  #print(', avg score=', score)\n",
        "  avg_score_list.append(score)\n",
        "env.close()\n",
        "\n",
        "# The following code is for making a plot of score vs. # lefts\n",
        "plt.plot(search_space, avg_score_list)\n",
        "plt.xlabel('Search Space (# lefts taken before going right)')\n",
        "plt.ylabel('avg # steps until done')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain what is going on in the figure above. Be sure to answer:\n",
        "  * What is the program doing generally?\n",
        "    * Answer: The program is running the mountain car simulation 10 times for each amount of steps to go left first. So the first 10 simulations, the mountain car goes left for only one step, then goes right for the rest of the sim and records the total nuber of steps taken. The next 10 simulations, the car will go left for two steps, then go right, and so on. Eventually we get a graph that shows the number of steps taken to reach the goal, cross-referenced with the number of steps taken to the left first. \n",
        "  * Why does it look so noisy?\n",
        "    * Answer: The graph looks \"noisy\" because the program took a relatively small number of simulations for each step value, only 10 per step. Less data makes a noisier graph.\n",
        "  * How can you make it look less noisy?\n",
        "    * Answer: We could make this graph less noisy by increasing the number of simulations per steps to the left, like 100 or 1000 instead of 10. This would most likely smooth the graph out.\n",
        "  * How can you speed up the evaluation?\n",
        "    * Answer: \n",
        "    1. Use a different programming language since python is typically slower\n",
        "    2. Take a smaller number of simulations per steps to the left, so you are running less simulations overall\n",
        "    3. Choose a certain range of steps to the left to simulate based on previous results. If there are ranges where the car never seems to reach the goal, then don't simulate those and this will speed up the evaluation\n",
        "\n",
        "1. Modify the code in the cell above so that it uses 1000 repeats at each search point (i.e. each value for num_lefts), and limits the search to the range of only 25 to 50 num_lefts.  \n",
        "\n",
        "1. Rerun the code with your changes and answer the following:\n",
        "  * What did you learn from the modifications?\n",
        "    * Answer: We now have a bell-shaped curve showing the results for each number of steps to the left. It appears that the fastest results are achieved when the car goes left for the first 35 to 42 steps. This is a more accurate average as well because we ran 1000 simulations for each number of steps to the left (25-50) instead of just 10 simulations.\n",
        "  * What were the benefits and disadvantages of your changes?\n",
        "  * **Benefits:** We have a more accurate representation of the average number of steps to reach the goal for each number of steps to the left first. The information is more reliable and outlines an area where the best performance can be expected.\n",
        "\n",
        "  * **Disadvantages:** We could only see results for moving the car to the left between the first 25 and 50 steps, so we don't have a  representation of ALL possible values to move to the left and their average time-step results.\n",
        "\n",
        "  * Do you think its possible to find a better **agent** (i.e. not a better search method)? If so, how does it operate?\n",
        "    * Answer: There could be a better agent. A closed loop agent would be able to \"see\" its environment and choose directions to go based on its position and velocity. This could make the agent adapt to every unique scenario and increase the speed at which it reaches its goal every time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5M_haolEnpRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Part III - Hill-climbing (25pts)**\n",
        "\n",
        "The following cell performs a randomized local search of the num_left space for agents of the type that go left for num_lefts and then go right until done. Think of each value of num_lefts as a point in the search space.\n",
        "\n",
        "####**Picking the next search point**\n",
        "The code randomly selects between either +1 and -1 for the next spot to evaluate. For example, if the code just finished evaluating num_lefts = 20, the next value of num_lefts to evaluate will be randomly picked between 19 and 21.\n",
        "\n",
        "####**Deciding whether to use the next search point**\n",
        "If the next search point results in a better (lower) score, the search will continue from the next point. If it is not better, the code will \"go back\" to the last value of num_lefts and try to randomly pick a search point again.\n",
        "\n",
        "####**Deciding when to stop**\n",
        "If the last 5 attempts of picking a next search point fail (i.e. result in a lower score than the current point), the search will stop.\n",
        "\n",
        "1. Modify the code below so that the search point is randomly selected from +3, +2, +1, -1, -2, or -3 from the current point. Run the code after the change. \n",
        "\n",
        "1. Modfiy the code so that instead of always rejecting a new point that results in a worse, you unstead flip a coin, and 50% of the time, accept it as the next new point. Run the code after this change.\n",
        "\n",
        "1. Answer the following questions:  \n",
        "  * How does the performance of each version of code compare? Is it what you expect?\n",
        "\n",
        "    The performance increased with each change in code. We gave the hill climb search more opportunities to escape a local min or max and chose to sometimes flip a coin on where to go next, allowing more area to be covered and yield better results.\n",
        "\n",
        "  * In search problems, there is often a tradeoff of chances of finding a better/best solution vs. the amount of time you spend (yielding yet another type of optimization problem of optimizing bang for buck of computing. Which parameters or techniques can you adjust to adjust this bang for buck?\n",
        "  \n",
        "    You can narrow your search to less overall checks to decrease time taken, but this would also decrease the reliability of your results. The more checks you want to do, the longer it will take to perform the search. You could narrow the search instead by checking a vast amount of points but then ending the search when the same point is checked an overall less amount of times, say a mximum of 3 checks per point."
      ],
      "metadata": {
        "id": "l774ib_mcF0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt  # This import is for plotting\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def pick_next_point(current_point):\n",
        "  deltas = [-3,-2,-1,1,2,3]\n",
        "  next_point = current_point + random.choice(deltas)\n",
        "  if next_point < 25:\n",
        "    next_point = 25\n",
        "  if next_point > 55:\n",
        "    next_point = 55\n",
        "  return next_point\n",
        "\n",
        "def eval_point(num_lefts, num_repeats, env):\n",
        "  \"\"\"Evaluates and returns the average delay until done when using num_lefts. \n",
        "  \"\"\"\n",
        "  result_list = []\n",
        "  for iteration in range(num_repeats):\n",
        "    observation = env.reset()\n",
        "    step_i = 0  # use this variable as a counter for each step taken\n",
        "    while True:    \n",
        "      if step_i < num_lefts:\n",
        "        action = 0\n",
        "      else:\n",
        "        action = 2\n",
        "      observation, reward, done, info = env.step(action) \n",
        "      step_i = step_i + 1\n",
        "\n",
        "      # if car reaches the flag, exit the loop\n",
        "      if done: \n",
        "        break;\n",
        "    result_list.append(step_i)\n",
        "\n",
        "  score = sum(result_list)/num_repeats\n",
        "  return score\n",
        "\n",
        "\n",
        "#---------------------------------------\n",
        "avg_score_list = [] \n",
        "\n",
        "env = gym.make(\"MountainCar-v0\")\n",
        "\n",
        "# randomly pick an starting value for num_lefts\n",
        "num_lefts = int(random.random()*100)\n",
        "search_complete = False\n",
        "num_repeats_for_eval = 50\n",
        "max_search_iter = 100\n",
        "score = eval_point(num_lefts, num_repeats_for_eval, env)\n",
        "\n",
        "search_i = 0\n",
        "while not search_complete:\n",
        "  #pick next point\n",
        "  last_num_lefts = num_lefts\n",
        "  last_score = score\n",
        "  num_lefts = pick_next_point(num_lefts)\n",
        "  print('search iter#', search_i, end='')\n",
        "  print('  current #lefts = ', num_lefts)\n",
        "\n",
        "  #evaluate this point\n",
        "  score = eval_point(num_lefts, num_repeats_for_eval, env)\n",
        "  print(', avg score=', score)\n",
        "  avg_score_list.append([num_lefts, score])\n",
        "  \n",
        "  #acceptance or rejection\n",
        "  if (score > last_score):\n",
        "    #if new point is worse than last point, flip a coin \n",
        "    x = random.randint(0,1)\n",
        "    if x == 0:\n",
        "      num_lefts = last_num_lefts\n",
        "\n",
        "  # evaluate if it is time to stop\n",
        "  search_i += 1\n",
        "  if search_i >= max_search_iter:\n",
        "    search_complete = True\n",
        "env.close()\n",
        "\n",
        "print(avg_score_list)\n",
        "avg_score = np.array(avg_score_list)\n",
        "# The following code is for making a plot of score vs. # lefts\n",
        "plt.scatter(avg_score[:,0], avg_score[:,1])\n",
        "plt.xlabel('Search Space (# lefts taken before going right)')\n",
        "plt.ylabel('avg # steps until done')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zh5nOsVjntEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Number Generator, returns either 0 or 1\n",
        "x = random.randint(0,1)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "y8-6Z2HXqItZ",
        "outputId": "401f88ed-2082-45ac-8f34-efb63ef0971c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Search - Steepest decent/Hill-climbing"
      ],
      "metadata": {
        "id": "La4_AMNRYq1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part IV - Adversarial Search (50pts)##\n",
        "\n",
        "An **adversarial search** is a search in a game in which there are multiple adversaries, such as tic tac toe, chess, backgammon, go, as well as games with more than two opponents, such as chinese checkers or risk.\n",
        "\n",
        "The **MiniMax Strategy** (aka MinMax and MaxiMin)\n",
        "is a agent and search strategy used not only in AI, but also game theory, which assumes the opponent will play a perfect next move, so the agent should play the next move which minimizes the opponent's best possible outcome. In other words, when it is the agent's turn, it must search ahead at least the next two moves, and pick the move that gives the opponent the worst best option. (Please take a moment to think about that and make sure you understand).\n",
        "\n",
        "\n",
        "###Tic Tac Toe\"\n",
        "\n",
        "\n",
        "1. (0pts) Run the code to see how the random_agent performs. Be sure to read the random_agent function and understand it.\n",
        "2. (25 pts) Finish implementing the function agent1, the look ahead 1 move agent. Take note of the performance provided by the eval functions.\n",
        "3. (25 pts) Finish implememting the function agent2, the look ahead by 2 minimax strategy. \n",
        "\n",
        "If you can't get the code working, provide an explanation of how you expect it should work and what you expect the results to be. If you get it working (and it is right, you get full credit). However, if there is a bug, in order to maximize your chances of partial credit, you can also add an explanation of how you think it should work and whether the results are what you expect."
      ],
      "metadata": {
        "id": "BMFisoMzYfL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code implements the TIC TAC TOE engine as well as a random_agent,\n",
        "# and code to run it over and over in order to evaluate it. You don't have to \n",
        "# read and understand all the code, but you should at least read the function\n",
        "# names, arguments, and each function's \"\"\" \"\"\" comments.\n",
        "import random\n",
        "\n",
        "# the 3-in-a-row cell combinations that result in a win\n",
        "WINS = ((0,1,2),(3,4,5),(6,7,8),(0,3,6),(1,4,7),(2,5,8),(0,4,8),(2,4,6))\n",
        "\n",
        "def valid_next_moves(board):\n",
        "  \"\"\"Returns a list of valid next moves given the board_state. \n",
        "  - Cells are referred by their 0-8 index (i.e. not 1-9). For example\n",
        "  - if the board is empty, it will return [0,1,2,3,4,5,6,7,8]. \"\"\"\n",
        "  moves_list = []\n",
        "  # cycle through all the cells. If it is empty, add i to moves_list\n",
        "  for i in range(9):\n",
        "    if board[i] == '-':\n",
        "      moves_list.append(i)\n",
        "  return moves_list\n",
        "\n",
        "def get_x_cells(board):\n",
        "  \"\"\"Returns a list of cells that have X's in them.\"\"\"\n",
        "  return [i for i,v in enumerate(board) if v=='X']\n",
        "\n",
        "def get_o_cells(board):\n",
        "  \"\"\"Returns a list of cells that have O's in them.\"\"\"\n",
        "  return [i for i,v in enumerate(board) if v=='O']\n",
        "  \n",
        "def is_x_winner(board):\n",
        "  \"\"\"Returns True if there are three X's in a row, otherwise False\"\"\"\n",
        "  x_cells = get_x_cells(board)\n",
        "  \n",
        "  for win in WINS:\n",
        "    if len(set(win) & set(x_cells)) == 3:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def is_o_winner(board):\n",
        "  \"\"\"Returns True if there are three O's in a row, otherwise False\"\"\"\n",
        "  o_cells = get_o_cells(board)\n",
        "  \n",
        "  for win in WINS:\n",
        "    if len(set(win) & set(o_cells)) == 3:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def is_game_over(board):\n",
        "  \"\"\" Returns 'X' if X won; 'O' if O won; 'CAT' if Cat's game, otherwise False. \n",
        "  \"\"\"\n",
        "  if is_x_winner(board):\n",
        "    return 'X'\n",
        "  elif is_o_winner(board):\n",
        "    return 'O'\n",
        "  elif valid_next_moves(board) == []:\n",
        "    return \"CAT\"\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "# RANDOM AGENT\n",
        "def random_agent(board):\n",
        "  \"\"\" Returns a random move out of valid next moves.\n",
        "  If no moves are left, produces an assertion error \"\"\"\n",
        "  moves = valid_next_moves(board)\n",
        "  if moves == []:\n",
        "    assert(False)\n",
        "  else:\n",
        "    return random.choice(moves)\n",
        "\n",
        "   \n",
        "\n",
        "def play(x_agent, o_agent):\n",
        "  \"\"\" Plays one game of tic_tac_toe calling the x_agent on x's turns and the \n",
        "  o_agent on o's turns.\n",
        "  Returns: \n",
        "    (winner, move_history, board_state)\n",
        "  where winner is one of {'X','O','CAT'}, move_history is a list of move\n",
        "  indices as they were played, and board_state is a list of the final board \n",
        "  contents. \n",
        "  \"\"\"\n",
        "  board_state = ['-']*9 # beginning empty board\n",
        "  move_history = []     # this list will keep track of all moves\n",
        "  while not is_game_over(board_state):\n",
        "    if (len(move_history) % 2) == 0:\n",
        "      next_move = x_agent(board_state)\n",
        "      board_state[next_move] = 'X'\n",
        "    else:\n",
        "      next_move = o_agent(board_state)\n",
        "      board_state[next_move] = 'O'\n",
        "    move_history += [next_move]\n",
        "\n",
        "  #You can uncomment any of the print statements to help debug your agent code.\n",
        "  #print('Game Over')\n",
        "  winner = is_game_over(board_state)\n",
        " # print('Winner is: ', winner)\n",
        " # print(str(board_state[0:3]))\n",
        " # print(str(board_state[3:6]))\n",
        " # print(str(board_state[6:9]))\n",
        " # print(move_history)\n",
        "  return (winner, move_history, board_state)\n",
        "\n",
        "def eval(x_agent,o_agent):\n",
        "  \"\"\"Runs 100 games with the designated agents and prints the number of wins \n",
        "  for x, o, and cat. \n",
        "  \"\"\"\n",
        "  x_wins, o_wins, cat_wins = 0, 0, 0\n",
        "  for i in range(100):\n",
        "    winner = play(x_agent, o_agent)[0] #the 0th list item returned is the winner\n",
        "    if(winner == 'X'):\n",
        "      x_wins += 1\n",
        "    elif(winner == 'O'):\n",
        "      o_wins += 1\n",
        "    elif(winner == 'CAT'):\n",
        "      cat_wins += 1\n",
        "    else:\n",
        "      assert(False)  # something bad happened if there is no winner\n",
        " # print('wins:')\n",
        "  print(' x:', x_wins) \n",
        "  print(' o:', o_wins)\n",
        "  print(' c:', cat_wins)\n",
        "\n",
        "\n",
        "eval(random_agent, random_agent)"
      ],
      "metadata": {
        "id": "4wibbzgUWtbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: finish implementing agent1, it currently just selects first valid move\n",
        "def agent1(board):\n",
        "  \"\"\"Cycles through all valid moves for the current board. If one results in a \n",
        "  win, it will return that move, otherwise, it will randomly pick a move. \"\"\"\n",
        "  moves = valid_next_moves(board)\n",
        "  if moves == []:\n",
        "    assert(False)\n",
        "\n",
        "  # check to see if it is X's turn or O's turn\n",
        "  if len(moves)%2:\n",
        "    player, opponent = 'X', 'O'\n",
        "    i_am_winner = is_x_winner\n",
        "  else:\n",
        "    player, opponent = 'O', 'X'\n",
        "    i_am_winner = is_o_winner\n",
        "\n",
        "  best_move = moves[0] # temporarily pick first valid move\n",
        "\n",
        "  # TODO: INSERT BELOW!!!\n",
        "  #If the next move is a win, choose that move\n",
        "  for move in moves:\n",
        "    test_board = board.copy()\n",
        "    # have player move into cell # move by modifying test_board\n",
        "    test_board[move] = player\n",
        "    if i_am_winner(test_board):\n",
        "      return move\n",
        "\n",
        "  #return random move if none result in a win\n",
        "      return random.choice(moves)\n",
        "\n",
        "\n",
        "\n",
        "  # HINTS: use a for loop, use the function i_am_winner(), use a temporary\n",
        "  # game board with the current move being evaluated added. A copy of the list\n",
        "  # can be made using the list.copy() function:\n",
        "  #   test_board = board.copy()\n",
        "  # When you find a winning move, return it immediately rather than continuing\n",
        "  # looping. \n",
        "  # TODO: INSERT ABOVE HERE!!!\n",
        "\n",
        "  return best_move\n",
        "\n",
        "eval(agent1, random_agent)\n",
        "eval(random_agent, agent1)"
      ],
      "metadata": {
        "id": "NlETmRvmMDKy",
        "outputId": "6e5e831a-ed7b-443b-da28-bb48c3fa5d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " x: 89\n",
            " o: 11\n",
            " c: 0\n",
            " x: 38\n",
            " o: 62\n",
            " c: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO, finish evaluating agent2, it currently just selects first valid move\n",
        "def agent2(board):\n",
        "  \"\"\"A minimal implementation of minimax strategy looking two moves ahead.\n",
        "  If a next move results in a win, it is returned. Otherwise, for each\n",
        "  player move, it will evaluate if the opponent can win. If there is a move\n",
        "  which doesn't allow the opponent to win, it will be selected.\n",
        "  \"\"\"\n",
        "  moves = valid_next_moves(board)\n",
        "  if moves == []:\n",
        "    assert(False)\n",
        "  if len(moves)%2:\n",
        "    player, opponent = 'X', 'O'\n",
        "    player_is_winner = is_x_winner\n",
        "    opponent_is_winner = is_o_winner\n",
        "  else:\n",
        "    player, opponent = 'O', 'X'\n",
        "    player_is_winner = is_o_winner\n",
        "    opponent_is_winner = is_x_winner\n",
        "\n",
        "  # for each valid move, hold's opponents best score\n",
        "  move_opponent_score_dict = {} \n",
        "\n",
        "  worst_move_for_opponent = moves[0] # temporarily pick first valid move\n",
        "\n",
        "  # TODO: INSERT CODE BELOW\n",
        "   #If the next move is a win, choose that move\n",
        "  for move in moves:\n",
        "    test_board = board.copy()\n",
        "    # have player move into cell # move by modifying test_board\n",
        "    test_board[move] = player\n",
        "    if player_is_winner(test_board):\n",
        "      return move\n",
        "\n",
        "    for move in moves:\n",
        "      test_board = board.copy()\n",
        "      test_board[move] = opponent\n",
        "      if opponent_is_winner(test_board):\n",
        "        return move\n",
        "  # HINTS: use a for loop within a for loop. The outer for loop will cycle\n",
        "  # over player moves, the inner for loop cycles over opponent moves possible\n",
        "  # after the current selected player move.\n",
        "  # Only keep track of two types of score for the opponent (1 for opponent win,\n",
        "  # 0 otherwise)\n",
        "  #return move with lowest opponent score\n",
        "  # TODO: INSERT CODE ABOVE\n",
        "  return worst_move_for_opponent\n",
        "\n",
        "\n",
        "eval(agent2, random_agent)\n",
        "eval(random_agent, agent2)"
      ],
      "metadata": {
        "id": "Dk__w7ZERwCj",
        "outputId": "6572438d-5fb7-4be4-bbcf-3a9ef7cf0d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " x: 87\n",
            " o: 5\n",
            " c: 8\n",
            " x: 11\n",
            " o: 63\n",
            " c: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FOR AGENT 2:\n",
        "\n",
        "I don't believe I coded this agent properly, *however*:\n",
        "\n",
        "When agent2 goes first it almost always wins (expected). When the random_agent goes first, agent2 wins around 70% of the time(expected). I would expect to see an overall increase in games that end in a draw when random_agent goes first, since agent2 will block the random_agent from winning whenever it can. This is indeed shown in the results with up to approx. 25% of games ending in draws when the random_agent goes first."
      ],
      "metadata": {
        "id": "nwbm5abm9Bg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#How to submit your project:\n",
        "\n",
        "1. Please change the name of the project to *your name, project 1*:\n",
        "  * for example:  \n",
        "    *  Taylan Sen, Project 1.ipynb\n",
        "1. Download the file as a ipynb file. Then upload it to canvas.\n",
        "\n",
        "Please remember, you can discuss the project amongst yourselves, but you must not copy and paste nor directly copy code from each other."
      ],
      "metadata": {
        "id": "juE2CRdvVyBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is some test code I used that you are welcom to repurpose\n",
        "\n",
        "def test_is_x_winner():\n",
        "  assert(is_x_winner(['X']*9))\n",
        "  assert(not is_x_winner(['O']*9))\n",
        "  assert(is_x_winner(['X', 'X', 'X', 'O','O','O','-','-','O']))\n",
        "  assert(not is_x_winner(['X', 'X', 'O', 'O','O','O','-','-','O']))\n",
        "\n",
        "def test_is_game_over():\n",
        "  assert(is_game_over(['X']*9) == 'X')\n",
        "  assert(is_game_over(['O']*9) == 'O')\n",
        "  assert(is_game_over(['-']*9) == False)\n",
        "  assert(is_game_over(['X', 'X', 'X', 'O','O','O','-','-','O']) == 'X')\n",
        "  assert(is_game_over(['X', 'X', 'O', 'O','O','O','-','-','O']) == 'O')\n",
        "  assert(is_game_over(['X', 'O', 'O',   'O','X','X',  'X','O','O']) == 'CAT')\n",
        "  assert(is_game_over(['X', 'X', 'O',   '-','O','O',  'X','O','X']) == False)\n",
        "\n",
        "test_is_x_winner()\n",
        "test_is_game_over()\n",
        "\n"
      ],
      "metadata": {
        "id": "o0gxX9G0UOhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}